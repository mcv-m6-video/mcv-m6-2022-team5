{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Multi-target single-camera (MTSC) tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import bz2\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from VehicleDetection import *\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# Import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2_dataset_loader import *\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.cuda.empty_cache()\n",
    "# PATHS\n",
    "DATASET = \"../datasets/aic19-track1-mtmc-train/train/\"\n",
    "SEQUENCES = [DATASET+seq+\"/\" for seq in os.listdir(DATASET)]\n",
    "CAMERAS = [[seq+cam+\"/\" for cam in os.listdir(seq)]for seq in SEQUENCES]\n",
    "SEQUENCES = [seq.replace(DATASET, \"\").replace(\"/\", \"\") for seq in SEQUENCES]\n",
    "CAMERAS = dict(zip(SEQUENCES, CAMERAS))\n",
    "\n",
    "# DEFINE SPLITS\n",
    "train = [\"S01\", \"S04\"]\n",
    "test = [\"S03\"]\n",
    "\n",
    "# Model Parameters\n",
    "selected_model = 'COCO-Detection/retinanet_R_101_FPN_3x.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video(path, div_frames, skip):\n",
    "    vidcap = cv2.VideoCapture(path)\n",
    "    fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
    "    num_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frames = []\n",
    "    # Read Half the frames \n",
    "    for _ in range(num_frames//div_frames):\n",
    "        for i in range(skip):\n",
    "            frame = vidcap.read()[1]\n",
    "            if i == 0:\n",
    "                frames.append(frame.astype(np.float32)) # Reduce soze\n",
    "    return iter(frames) # Iterator\n",
    "\n",
    "def readDetections(path):\n",
    "  #Generates detection dictionary where the frame number is the key and the values are the info of the corresponding detection/s\n",
    "  \n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    detections = {}\n",
    "    for line in lines:\n",
    "        data = line.split(',')\n",
    "        if data[0] in detections:\n",
    "            detections[data[0]].append(VehicleDetection(int(data[0]), int(data[1]), float(data[2]), float(data[3]), float(data[4]), float(data[5]), float(data[6])))\n",
    "        else:\n",
    "            detections[data[0]] = [VehicleDetection(int(data[0]), int(data[1]), float(data[2]), float(data[3]), float(data[4]), float(data[5]), float(data[6]))]\n",
    "\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing S03...: 100%|██████████| 6/6 [00:32<00:00,  5.39s/it]\n"
     ]
    }
   ],
   "source": [
    "seq_data = []\n",
    "\n",
    "# For each training seq move through cameras and extact even frames and even gt\n",
    "for i, seq in enumerate(test):\n",
    "    for j, cam in tqdm(enumerate(CAMERAS[seq]), total = len(CAMERAS[seq]), desc = f\"Processing {seq}...\"):\n",
    "        data = {}\n",
    "        data[\"div\"] = 1\n",
    "        data[\"base_path\"] = cam + \"frames/\" # To Save Frames\n",
    "        data[\"gt_detected\"] = readDetections(cam + \"gt/gt.txt\")\n",
    "        data[\"gt_detected\"] = {key:data[\"gt_detected\"][key] for key in data[\"gt_detected\"].keys() if int(key) % data[\"div\"] == 0}\n",
    "        data[\"frames\"] = extract_video(cam + \"vdo.avi\", 10,data[\"div\"])\n",
    "        seq_data.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44518"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DatasetCatalog.clear()\n",
    "DatasetCatalog.register(\"AICity_eval\" , lambda d=seq_data: get_AICity_dicts_big(d))\n",
    "MetadataCatalog.get(\"AICity_eval\").set(thing_classes=[\"car\"])\n",
    "AICity_metadata = MetadataCatalog.get(\"AICity_eval\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading config /home/group05/anaconda3/lib/python3.7/site-packages/detectron2/model_zoo/configs/COCO-Detection/../Base-RetinaNet.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/06 10:21:34 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|    car     | 489          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[04/06 10:21:34 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[04/06 10:21:34 d2.data.common]: \u001b[0mSerializing 481 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/06 10:21:34 d2.data.common]: \u001b[0mSerialized dataset takes 0.13 MiB\n",
      "\u001b[32m[04/06 10:21:34 d2.evaluation.evaluator]: \u001b[0mStart inference on 481 batches\n",
      "\u001b[32m[04/06 10:21:35 d2.evaluation.evaluator]: \u001b[0mInference done 11/481. Dataloading: 0.0117 s/iter. Inference: 0.0447 s/iter. Eval: 0.0002 s/iter. Total: 0.0566 s/iter. ETA=0:00:26\n",
      "\u001b[32m[04/06 10:21:40 d2.evaluation.evaluator]: \u001b[0mInference done 91/481. Dataloading: 0.0172 s/iter. Inference: 0.0448 s/iter. Eval: 0.0002 s/iter. Total: 0.0622 s/iter. ETA=0:00:24\n",
      "\u001b[32m[04/06 10:21:45 d2.evaluation.evaluator]: \u001b[0mInference done 171/481. Dataloading: 0.0173 s/iter. Inference: 0.0449 s/iter. Eval: 0.0002 s/iter. Total: 0.0625 s/iter. ETA=0:00:19\n",
      "\u001b[32m[04/06 10:21:50 d2.evaluation.evaluator]: \u001b[0mInference done 252/481. Dataloading: 0.0173 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0625 s/iter. ETA=0:00:14\n",
      "\u001b[32m[04/06 10:21:55 d2.evaluation.evaluator]: \u001b[0mInference done 314/481. Dataloading: 0.0218 s/iter. Inference: 0.0448 s/iter. Eval: 0.0002 s/iter. Total: 0.0668 s/iter. ETA=0:00:11\n",
      "\u001b[32m[04/06 10:22:00 d2.evaluation.evaluator]: \u001b[0mInference done 354/481. Dataloading: 0.0287 s/iter. Inference: 0.0445 s/iter. Eval: 0.0002 s/iter. Total: 0.0735 s/iter. ETA=0:00:09\n",
      "\u001b[32m[04/06 10:22:05 d2.evaluation.evaluator]: \u001b[0mInference done 436/481. Dataloading: 0.0263 s/iter. Inference: 0.0447 s/iter. Eval: 0.0002 s/iter. Total: 0.0712 s/iter. ETA=0:00:03\n",
      "\u001b[32m[04/06 10:22:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:33.701865 (0.070802 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/06 10:22:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:21 (0.044675 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/06 10:22:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[04/06 10:22:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./results_train_seq01-04/coco_instances_results.json\n",
      "\u001b[32m[04/06 10:22:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Results do not correspond to current coco set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2038dec0cb26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCOEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AICity_eval\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_detection_test_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AICity_eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm -rf last_id.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/detectron2/evaluation/evaluator.py\u001b[0m in \u001b[0;36minference_on_dataset\u001b[0;34m(model, data_loader, evaluator)\u001b[0m\n\u001b[1;32m    202\u001b[0m     )\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;31m# An evaluator may return None when not in main process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;31m# Replace it by an empty dict instead to make it easier for downstream code to handle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/detectron2/evaluation/coco_evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, img_ids)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_box_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"instances\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Copy so the caller can do whatever with results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/detectron2/evaluation/coco_evaluation.py\u001b[0m in \u001b[0;36m_eval_predictions\u001b[0;34m(self, predictions, img_ids)\u001b[0m\n\u001b[1;32m    273\u001b[0m                     \u001b[0mmax_dets_per_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_dets_per_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 )\n\u001b[0;32m--> 275\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoco_results\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# cocoapi does not handle empty results very well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/detectron2/evaluation/coco_evaluation.py\u001b[0m in \u001b[0;36m_evaluate_predictions_on_coco\u001b[0;34m(coco_gt, coco_results, iou_type, kpt_oks_sigmas, use_fast_impl, img_ids, max_dets_per_image)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m     \u001b[0mcoco_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoco_gt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadRes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoco_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m     \u001b[0mcoco_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCOCOeval_opt\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_fast_impl\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mCOCOeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoco_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoco_dt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;31m# For COCO, the default max_dets_per_image is [1, 10, 100].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36mloadRes\u001b[0;34m(self, resFile)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mannsImgIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mann\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannsImgIds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannsImgIds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetImgIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                \u001b[0;34m'Results do not correspond to current coco set'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'caption'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mimgIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mann\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Results do not correspond to current coco set"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(selected_model))\n",
    "cfg.DATASETS.VAL = ('AICity_eval',)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(selected_model)  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 1e-3\n",
    "cfg.SOLVER.MAX_ITER = 5000\n",
    "cfg.SOLVER.STEPS = [] # do not decay learning rate\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 # (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.BACKBONE.FREEZE_AT = 1\n",
    "\n",
    "cfg.OUTPUT_DIR = \"./results_train_seq01-04\"\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "evaluator = COCOEvaluator(\"AICity_eval\", output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, \"AICity_eval\")\n",
    "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
    "!rm -rf last_id.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "441f105d8e40ce6e65578ed79f7ab413fda8686854e997d7b6b6eaeb5bf38176"
  },
  "kernelspec": {
   "display_name": "m5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
