{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Multi-target single-camera (MTSC) tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import bz2\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from VehicleDetection import *\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "from pqdm.processes import pqdm\n",
    "\n",
    "\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# Import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2_dataset_loader import *\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.cuda.empty_cache()\n",
    "# PATHS\n",
    "DATASET = \"../datasets/aic19-track1-mtmc-train/train/\"\n",
    "SEQUENCES = [DATASET+seq+\"/\" for seq in os.listdir(DATASET)]\n",
    "CAMERAS = [[seq+cam+\"/\" for cam in os.listdir(seq)]for seq in SEQUENCES]\n",
    "SEQUENCES = [seq.replace(DATASET, \"\").replace(\"/\", \"\") for seq in SEQUENCES]\n",
    "CAMERAS = dict(zip(SEQUENCES, CAMERAS))\n",
    "\n",
    "# DEFINE SPLITS\n",
    "train = [\"S01\", \"S04\"]\n",
    "test = [\"S03\"]\n",
    "\n",
    "# Model Parameters\n",
    "selected_model = 'COCO-Detection/retinanet_R_101_FPN_3x.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video(path, div_frames, skip):\n",
    "    vidcap = cv2.VideoCapture(path)\n",
    "    fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
    "    num_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frames = []\n",
    "    # Read Half the frames \n",
    "    for _ in range(num_frames//div_frames):\n",
    "        for i in range(skip):\n",
    "            frame = vidcap.read()[1]\n",
    "            if i == 0:\n",
    "                frames.append(frame.astype(np.float32)) # Reduce soze\n",
    "    return iter(frames) # Iterator\n",
    "\n",
    "def readDetections(path):\n",
    "  #Generates detection dictionary where the frame number is the key and the values are the info of the corresponding detection/s\n",
    "  \n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    detections = {}\n",
    "    for line in lines:\n",
    "        data = line.split(',')\n",
    "        if data[0] in detections:\n",
    "            detections[data[0]].append(VehicleDetection(int(data[0]), int(data[1]), float(data[2]), float(data[3]), float(data[4]), float(data[5]), float(data[6])))\n",
    "        else:\n",
    "            detections[data[0]] = [VehicleDetection(int(data[0]), int(data[1]), float(data[2]), float(data[3]), float(data[4]), float(data[5]), float(data[6]))]\n",
    "\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing S04...: 100%|██████████| 25/25 [00:05<00:00,  4.76it/s]\n"
     ]
    }
   ],
   "source": [
    "seq_data = []\n",
    "\n",
    "# For each training seq move through cameras and extact even frames and even gt\n",
    "for i, seq in enumerate(test):\n",
    "    for j, cam in tqdm(enumerate(CAMERAS[seq]), total = len(CAMERAS[seq]), desc = f\"Processing {seq}...\"):\n",
    "        data = {}\n",
    "        data[\"div\"] = 1\n",
    "        data[\"base_path\"] = cam + \"frames/\" # To Save Frames\n",
    "        data[\"gt_detected\"] = readDetections(cam + \"gt/gt.txt\")\n",
    "        data[\"gt_detected\"] = {key:data[\"gt_detected\"][key] for key in data[\"gt_detected\"].keys() if int(key) % data[\"div\"] == 0}\n",
    "        data[\"frames\"] = extract_video(cam + \"vdo.avi\", 25,data[\"div\"])\n",
    "        seq_data.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8119"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DatasetCatalog.clear()\n",
    "DatasetCatalog.register(\"AICity_eval\" , lambda d=seq_data: get_AICity_dicts_big(d))\n",
    "MetadataCatalog.get(\"AICity_eval\").set(thing_classes=[\"car\"])\n",
    "AICity_metadata = MetadataCatalog.get(\"AICity_eval\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading config /home/yusepp/anaconda3/envs/m5/lib/python3.8/site-packages/detectron2/model_zoo/configs/COCO-Detection/../Base-RetinaNet.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/06 04:37:35 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[32m[04/06 04:37:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[04/06 04:37:49 d2.data.common]: \u001b[0mSerializing 166 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/06 04:37:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.05 MiB\n",
      "\u001b[32m[04/06 04:37:49 d2.evaluation.evaluator]: \u001b[0mStart inference on 166 batches\n",
      "\u001b[32m[04/06 04:37:50 d2.evaluation.evaluator]: \u001b[0mInference done 11/166. Dataloading: 0.0010 s/iter. Inference: 0.0697 s/iter. Eval: 0.0002 s/iter. Total: 0.0709 s/iter. ETA=0:00:10\n",
      "\u001b[32m[04/06 04:37:55 d2.evaluation.evaluator]: \u001b[0mInference done 85/166. Dataloading: 0.0043 s/iter. Inference: 0.0635 s/iter. Eval: 0.0002 s/iter. Total: 0.0680 s/iter. ETA=0:00:05\n",
      "\u001b[32m[04/06 04:38:01 d2.evaluation.evaluator]: \u001b[0mInference done 158/166. Dataloading: 0.0061 s/iter. Inference: 0.0622 s/iter. Eval: 0.0002 s/iter. Total: 0.0685 s/iter. ETA=0:00:00\n",
      "\u001b[32m[04/06 04:38:01 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:11.075910 (0.068794 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/06 04:38:01 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:09 (0.062074 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/06 04:38:01 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[04/06 04:38:01 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./results/coco_instances_results.json\n",
      "\u001b[32m[04/06 04:38:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.20s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.546\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.593\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.585\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.667\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.545\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.589\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.706\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.900\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.705\n",
      "\u001b[32m[04/06 04:38:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 54.564 | 59.326 | 58.505 |  nan  | 66.667 | 54.505 |\n",
      "\u001b[32m[04/06 04:38:01 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "OrderedDict([('bbox', {'AP': 54.56430627984741, 'AP50': 59.32580510213739, 'AP75': 58.50497668653435, 'APs': nan, 'APm': 66.66666666666666, 'APl': 54.50536704668064})])\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(selected_model))\n",
    "cfg.DATASETS.VAL = ('AICity_eval',)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(selected_model)  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 1e-3\n",
    "cfg.SOLVER.MAX_ITER = 5000\n",
    "cfg.SOLVER.STEPS = [] # do not decay learning rate\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 # (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
    "cfg.MODEL.BACKBONE.FREEZE_AT = 1\n",
    "\n",
    "cfg.OUTPUT_DIR = \"./results\"\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "evaluator = COCOEvaluator(\"AICity_eval\", output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, \"AICity_eval\")\n",
    "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
    "!rm -rf last_id.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "441f105d8e40ce6e65578ed79f7ab413fda8686854e997d7b6b6eaeb5bf38176"
  },
  "kernelspec": {
   "display_name": "m5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
